\documentclass[a4paper]{article} 

\addtolength{\hoffset}{-2.25cm}
\addtolength{\textwidth}{4.5cm}
\addtolength{\voffset}{-3.25cm}
\addtolength{\textheight}{5cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0in}

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\usepackage{blindtext} % Package to generate dummy text
\usepackage{charter} % Use the Charter font
\usepackage[utf8]{inputenc} % Use UTF-8 encoding
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage[english, ngerman]{babel} % Language hyphenation and typographical rules
\usepackage{amsthm, amsmath, amssymb} % Mathematical typesetting
\usepackage{float} % Improved interface for floating objects
\usepackage[final, colorlinks = true,
            linkcolor = black,
            citecolor = black]{hyperref} % For hyperlinks in the PDF
\usepackage{graphicx, multicol} % Enhanced support for graphics
\usepackage{xcolor} % Driver-independent color extensions
\usepackage{marvosym, wasysym} % More symbols
\usepackage{rotating} % Rotation tools
\usepackage{censor} % Facilities for controlling restricted text
\usepackage{listings} % Environment for non-formatted code, !uses style file!
\usepackage{pseudocode} % Environment for specifying algorithms in a natural way
 % Environment for f-structures, !uses style file!
\usepackage{booktabs} % Enhances quality of tables
\usepackage{tikz-qtree} % Easy tree drawing tool
 % Configuration for b-trees and b+-trees, !uses style file!
\usepackage[backend=biber,style=numeric,
            sorting=nyt]{biblatex} % Complete reimplementation of bibliographic facilities
\addbibresource{ecl.bib}
\usepackage{csquotes} % Context sensitive quotation facilities
\usepackage[yyyymmdd]{datetime} % Uses YEAR-MONTH-DAY format for dates
\renewcommand{\dateseparator}{-} % Sets dateseparator to '-'
\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{}\renewcommand{\headrulewidth}{0pt} % Blank out the default header
\fancyfoot[L]{} % Custom footer text
\fancyfoot[C]{} % Custom footer text
\fancyfoot[R]{\thepage} % Custom footer text
\newcommand{\note}[1]{\marginpar{\scriptsize \textcolor{red}{#1}}} % Enables comments in red on margin
\usepackage{mathtools}
\usepackage{amsmath}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\usepackage{cancel}
%-------------------------------

%----------------------------------------------------------------------------------------

%-------------------------------
%	ENVIRONMENT SECTION
%-------------------------------
\pagestyle{fancy}
\usepackage{mdframed}


\newenvironment{problem}[2][Problema]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
    {\textit{Solución}}
    {}


%-------------------------------------------------------------------------------------------
%	CUSTOM COMMANDS
%-------------------------------
\newcommand{\gaussian}{\frac{1}{\sigma\sqrt{2\pi}}\exp\left(- \frac{(x-\mu)^2}{2\sigma^2}\right)}
\newcommand{\R}{\mathbb R}



\begin{document}



%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Javier Sáez \hfill\\   
77448344F \hfill\\
franciscojavier.saez@estudiante.uam.es
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Ejercicios\\ 
\normalsize 
Teoría de la Información\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------


\begin{problem}{1}
Sea $X \sim \mathcal N(\mu, \sigma^2)$ para ciertos $\mu,\sigma^2 \in \mathbb R$. Calcula su entropía.\\
\end{problem}

Recordamos que la función de densidad de una variable aleatoria que sigue una distribución gaussiana de parámetros $\mu,\sigma^2$ es
\[
f(x) = \gaussian ,    
\]
y que la entropía de una variable aleatoria continua $X$ en general viene dada por la expresión:
\[
H(X) = - \mathbb E \left[\log X\right] = \int_{-\infty}^\infty f_X(x) \left(-\log f_X(x)\right) dx.
\]
Podemos resolver esta integral de forma muy sencilla si sustituimos la función de densidad $f$ en la parte del logaritmo pero no en la parte que aparece sola. Tenemos que
\[
-\log f(x) = \log \frac{1}{f(x)} = \log  \left(2 \pi \sigma^2)^{\frac{1}{2}} \exp \left( \frac{1}{2\sigma^2}(x-\mu)^2 \right) \right) = \frac{1}{2} \log \left( 2\pi \sigma^2 \right) + \frac{1}{2\sigma^2} (x-\mu)^2
\]
Con esto se tiene que, en nuestro caso:
\begin{align*}
  H(X) %& = - \int \gaussian \log \left(\gaussian\right) dx  \\
  %& = \int_{-\infty}^\infty f_X(x) \left( \frac{1}{2} \log \left(2\pi \sigma^2\right) + \frac{1}{2\sigma^2} (x-\mu)^2 \right) \\
  & = \frac{1}{2} \log \left(2\pi \sigma^2\right) \underbrace{\int_{-\infty}^\infty f_X(x) dx}_{(1)} +  \frac{1}{2\sigma^2}  \underbrace{\int_{-\infty}^\infty f_X(x)(x-\mu)^2 dx}_{(2)}
%& \stackrel{(1)}{=}  \frac{1}{\sigma \sqrt{2\pi}} \int \exp\left(- \frac{(x-\mu)^2}{2\sigma^2}\right) \log \left(\sigma \sqrt{2\pi} \exp \left( \frac{(x-\mu)^2}{2\sigma^2}\right) \right)   dx \\
\end{align*}
Donde vemos que en $(1)$ estamos integrando en todo el dominio posible la función de densidad de la normal, por lo que tenemos que eso es igual a $1$. Además, en $(2)$ tenemos la definición de la varianza $\nu = \sigma^2$ de la distribución normal de media $\mu$ y desviación típica $\sigma$.
\[
H(X) = \frac{1}{2} \log \left(2\pi \sigma^2\right) +  \frac{\sigma^2}{2\sigma^2} = \frac{1}{2}\left(\log \left(2\pi \sigma^2\right) + 1 \right) = \frac{1}{2}\left(\log \left(2\pi \sigma^2\right) + \log(e) \right) = \frac{1}{2}\log \left(2\pi e \sigma^2\right)  
\]
\qed


\begin{problem}{2}

 Sean $X,Y$ dos variables aleatorias. Probar que la información mutua entre ambas es simétrica, es decir, $MI(X;Y) = MI(Y;X)$.\\

\end{problem}

Este ejercicio es una consecuencia prácticamente trivial de que la distribución conjunta de $P(X,Y)$ es la misma que la de $P(Y,X)$. Lo vemos formalmente. Recordamos que
\begin{equation}
  \label{MI:discreta}
MI(X;Y) = \sum_{x,y} P_{XY}(x,y) \log \frac{P_{XY}(x,y)}{P_X(x)P_Y(y)}
\end{equation}
Ahora, basta cambiar el orden de las sumatorias  y aplicar que $P(X,Y) = P(Y,X)$ para ver que:
\[
MI(X;Y) = \sum_{x,y} P_{XY}(x,y) \log \frac{P_{XY}(x,y)}{P_X(x)P_Y(y)} = \sum_{y,x} P_{YX}(y,x) \log \frac{P_{YX}(y,x)}{P_Y(y)P_X(x))} = MI(Y;X)
    \]

    como queríamos ver.

En el caso continuo, la demostración es análoga, pues podemos aprovecharnos de Teorema de Fubini para cambiar las integrales y obtener el mismo resultado.

\qed

\begin{problem}{3}

 Sean $X,Y$ dos variables aleatorias. Demostrar que, si $H(X)$ es la entropía de $X$, y $H(X,Y)$ es la entropía conjunta de $X$ e $Y$, entonces:
\[
MI(X;Y) = H(X) + H(Y) - H(X,Y).
\]
\end{problem}

Lo primero que hacemos es recordar la definición de entropía y entropía conjunta para variables discretas. Tenemos que
\begin{align}
  H(X) & = - \mathbb E \left[\log X\right] \label{entropia} \\
  H(X,Y) & = - \mathbb E \left[ \log P_{XY}(x,y)\right] \label{entropia:conjunta}
\end{align}
El resultado que queremos obtener se sigue de operar en la definición de información mutua \eqref{MI:discreta}. Lo primero que haremos es separar el logaritmo usando las propiedades de logaritmo del producto y del cociente:
\begin{align}
  MI(X;Y) &  = \sum_{x,y} P_{XY}(x,y) \log \frac{P_{XY}(x,y)}{P_X(x)P_Y(y)} \nonumber \\
  & = \sum_{x,y} P_{XY}(x,y) \left( \log P_{XY}(x,y) - \log P_X(x) - \log P_Y(y) \right)\nonumber \\
  & =  \underbrace{\sum_{x,y}P_{XY}(x,y) \log P_{XY}(x,y)}_{-H(X,Y)} - \sum_{x,y} P_{XY}(x,y) \log P_X(x) - \sum_{x,y} P_{XY}(x,y) \log P_Y(y) \label{ej3:align}
\end{align}
Ya hemos obtenido uno de los términos que queríamos. Ahora, basta ver que
\[
\sum_x \sum_y P_{XY}(x,y) \log P_X(x) = \sum_x \log P_X(x) \sum_y P_{XY}(x,y) \stackrel{(*)}{=} \sum_x P_X(x) \log P_X(x) = - H(X)
\]
donde en $(*)$ hemos usado que estamos sumando para un $x$ fijo sobre todos los valores de $y$, luego nos queda simplemente la probabilidad de ese $x$ en la distribución marginal $P_X$. Así, sustiyuyendo esto en la ecuación \eqref{ej3:align}, obtemos lo que queríamos:
$$
MI(X;Y) = -H(X,Y) - (- H(X)) - (-H(Y)) = H(X) + H(Y) - H(X,Y)
$$

\qed

\begin{problem}{4}
 Sean $X,Y$ dos variables aleatorias. Con la notación del ejercicio anterior, demostrar que:
\begin{itemize}
\item $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$
\item $MI(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) $
\item $MI(X,X) = H(X)$
\end{itemize}

\end{problem}

Recordamos primero la definición de entropía condicionada:
\begin{equation}\label{entropia:condicionada}
  H(X|Y) = - \mathbb E_{P_{XY}} \left[\log \frac{P_{XY}(x,y)}{P_X(x)} \right] = - \sum_{x,y} P_{XY}(x,y)\log \frac{P_{XY}(x,y)}{P_X(x)} = - \sum_{x} P_X(x) \sum_y P_{Y|X}(y|x)\log P_{Y|X}(y|x)
\end{equation}

Vamos a usar esta definición y las que se han dado en ejercicios anteriores para probar estas igualdades. 

\begin{itemize}
\item $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$.\\

  Usando la definición de la entropía conjunta dada en la ecuación \eqref{entropia:conjunta}, y aplicando la definición de probabilidad condicionada $P_{XY}(x,y) = P_{Y|X}(y|x)P_X(x)$ en varias ocasiones obtenemos:
  \begin{align*}
    H(X,Y) & = -\sum_{x,y}P_{XY}(x,y) \log P_{XY}(x,y) \\
    & = - \sum_{x,y} P_{XY}(x,y)\log\left( P_{Y|X}(y|x)P_X(x)\right)\\
    & = - \sum_{x,y} P_{XY}(x,y)\left( \log P_{Y|X}(y|x) + \log P_X(x)\right)\\
    & = - \sum_{x,y} P_{XY}(x,y)\log P_{Y|X}(y|x)  - \sum_{x,y} P_{XY}(x,y)\log P_X(x)\\
    & =  - \sum_{x}P_X(x)\log P_X(x) - \sum_{x,y} P_{XY}(x,y) \log P_{Y|X}(y|x)\\
    & = H(X) - \sum_{x}P_X(x) \sum_{y} P_{Y|X}(y|x) \log P_{Y|X}(y|x) \\
    & = H(X) + H(Y|X)
  \end{align*}

  Del mismo modo pero usando la otra descomposición de la probabilidad conjunta, obtenemos la segunda igualdad.

\item $MI(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$.\\

  Para esta, podemos usar que hemos probado en el ejercicio anterior que $MI(X;Y) = H(X) + H(Y) - H(X,Y)$ y que en el apartado anterior hemos probado que $H(X,Y) = H(Y) + H(X|Y)$ para obtener que
  $$
  MI(X;Y) = H(X) + H(Y) - H(X,Y) = H(X) + H(Y) - (H(Y) + H(X|Y)) = H(X) - H(X|Y),
  $$
  y, del mismo modo podemos probar la otra igualdad usando que $H(X,Y) = H(X) + H(Y|X)$.
\item $ MI(X;X) = H(X)$.\\
  Este es consecuencia directa del anterior y de que
  \[
  H(X|X) = - E_{P_X} \left[\log \frac{P_X(x)}{P_X(x)} \right] =  - E_{P_X} \left[\log 1\right] = 0.
  \]
\end{itemize}
quedando así probadas todas las igualdades \qed


\begin{problem}{5}
En el contexto del \emph{temporal coding}, demostrar que, cuando \(\Delta t\ << \frac{1}{r}\), entonces la entropía viene dada por
\[
H \approx T r \log_2 \left(\frac{e}{r\Delta t}\right).  
\]
\end{problem}

Recordemos primero algunos conceptos del contexto en el que estamos:

\begin{itemize}
\item \(T\) es la longitud que tiene un tren de spikes, es un número fijo.
\item La longitud anteior está dividida en ventanas de tamaño \(\Delta t\), con lo que tenemos que $M = T/\Delta t$ ventanas.
\item Dentro de una ventana, se codifica un spike si encontramos un \(1\) en la ventana y un $0$ si no hay spike en esa ventana. Por tanto, nuestras observaciones serán vectores de longitud \(M\).
\item \(r\) será el \emph{firing rate}, que determina el número de unos y ceros que hay en un tren de disparos completo. Formalmente,
\[
  r = \frac{\#spikes}{T}.
  \]
\end{itemize}

Ahora, como tenemos \(M\) ventanas y \(\#spikes\), usando la definición de \(r\) la probabilidad de tener un spike en una ventana es:
\[
P = \frac{\#spikes}{M} = \frac{\#spikes}{T/\Delta t} = r \Delta t 
\]

Llegados a este punto, queremos suponer que la probabilidad de encontrar un spike es muy baja, es decir que
\[
P = r \Delta t << 1  ,
\]
para tener las condiciones que se nos indican en el ejercicio. Esto se puede conseguir siempre si tomamos una ventana muy pequeña en unos datos de gran dimensionalidad. 

Hemos de suponer que nuestra cadena es completamente aleatoria extraída del conjunto de todas las cadenas que tienen el mismo número de ceros y de unos. Así,
\[
P(x_i) = \frac{1}{{M \choose Tr}}, \quad i = 1,\dots, {M \choose Tr} 
\]

Como tenemos sucesos con la misma probabilidad, tenemos que la entropía viene dada por el logaritmo del total de los eventos. Llamando \(M_1 = Tr \) y \(M_0 = M - M_1\) a los números de 1 y 0 respectivamente, obtenemos aplicando propiedades del logaritmo que:
\[
H = \log_2 {M \choose M_1} = \log_2 \left( \frac{M!}{M_0! M_1!} \right) = \frac{1}{\log 2} \left(\log M! - \log M_0! - \log M_1 !\right).
\]

Ahora, si consideramos que las observaciones tienen dimensión suficientemente grande, podemos aproximar el logaritmo de un factorial usando la \emph{fórmula de Stirling}:
\[
\log n! \approx n \log n  - n = n(\log n - 1),
\]
y la usamos en la ecuación anterior obtenemos que:
\[
H \approx \frac{1}{\log 2} \left(M\log M - M - M_0\log M_0 - M_0 - M_1\log M_1 - M_1\right) 
\]
Y, como sabemos que \(M = M_1 + M_0\), entonces, podemos continuar operando del siguiente modo:
\begin{align*}
H &\approx \frac{1}{\log 2} \left(M\log M - M - M_0\log M_0 - M_0 - M_1\log M_1 - M_1\right) \\
& = \frac{1}{\log 2} \left(M_1(\log M - \log M_1) + M_0(\log M - \log M_0)\right)\\
& = -\frac{M}{\log 2} \left( (M_1/M)\log (M_1/M) + (M_0/M)\log(M_0/M)\right)
\end{align*}
Aplicando podemos expresar \(P = M_1/M\) primero y las definiciones iniciales después, obtenemos la siguiente cadena de igualdades:
\begin{align*}
H & \approx   -\frac{M}{\log 2} \left( P \log P + (1-P) \log(1-P) \right)\\
& = - \frac{T}{\Delta t \log 2} \left(r\Delta t \log (r\Delta t) + (1-r\Delta t)\log (1-r\Delta t) \right) \\
& \stackrel{(1)}{=} -\frac{T}{\Delta t \log 2} \left(r\Delta t \log (r\Delta t) + (1-r\Delta t)(-r\Delta t) \right)\\
& \approx -\frac{T}{\Delta t \log 2} \left(r\Delta t \log (r\Delta t) - r\Delta t \right) \\
& = - \frac{T}{\Delta t \log 2} r \Delta t \left(\log(r\Delta t) - 1\right)\\
& = - \frac{Tr}{ \log 2} r \Delta t \left(\log(r\Delta t) - \log e\right) \\
& = - \frac{Tr}{ \log 2} r \Delta t \left(\log \left( \frac{r\Delta t}{e}\right)\right)\\
& =  \frac{Tr}{ \log 2} r \Delta t \left(\log \left( \frac{e}{r\Delta t}\right)\right) \\
& = Tr \log_2\left( \frac{e}{r\Delta t}\right)
\end{align*}
donde en \((1)\) hemos usado el desarrollo en serie de Taylor de \( \log(1+x)\) despreciando el resto de Taylor. Llegamos así al resultado que queríamos demostrar. 
\qed

\begin{problem}{6}
Demostrar que la regla de la cadena de la entropía se puede extender a varias variables de la forma:
\[
H(X,Y|Z) = H(X|Z) +H(Y|X,Z).
\]
\end{problem}

Vemos primero que, usando la regla de Bayes, obtenemos:
\[
P(X,Y|Z) = P(X | Z) P(Y|X,Z).   
\]
Por lo que, podemos aplicar el logaritmo para obtener que:
\[
 \log P(X,Y|Z) = \log P(X | Z)+ \log P(Y|X,Z). 
\]
Ahora,aplicando esto, usando la fórmula de la entropía conjunta y sabiendo que tenemos que condicionar a una variable, obtenemos que
\begin{align*}
H(X,Y|Z) & = \sum_{x,y,z} P(x,y,z) \log \frac{1}{P(x,y|z)}\\
& = - \sum_{x,y,z} P(x,y,z) \log P(x,y|z) \\
& = - \sum_{x,y,z} P(x,y,z) \left(\log P(x|z) + \log P(y|x,z)\right)\\
& = - \sum_{x,y,z} P(x,y,z) \log P(x|z) - \sum_{x,y,z} P(x,y,z) \log P(y|x,z)\\
& = - \sum_{x,z} P(x,z) \log P(x|z) - \sum_{x,y,z} P(x,y,z) \log P(y|x,z)\\
& = H(X|Z) + H(Y|X,Z)
\end{align*}
que es lo que queríamos demostrar. \qed
\end{document}
