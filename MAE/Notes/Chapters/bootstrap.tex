\[
P(|\hat \theta_n - \theta | > \epsilon) = P(|\hat \theta_n - \theta |^2 > \epsilon^2) \leq \frac{E|\hat \theta - \theta|^2}{\epsilon^2} = \frac{sesgo^2(\hat\theta) + Var(\hat \theta)}{\epsilon^2} 
\]



Si tenemos 
\[
\sqrt{n} (\bar{X_n} - \mu)\to^d N(0,\sigma^2),
\]
 como \(\sqrt{n} \to \infty\) y la diferencia tiende a cero, nos está indicando que la velocidad con la que \(\bar{X_n}\) se acerca a \(\mu\) es la misma con la que \(\frac{1}{\sqrt{n}}\) va a cero




Sean \(X_1,\dots,X_n \sim U(0,\theta)\). Sabemos que \(\mu = \theta/2$ y que $\sigma^2 = \theta^2 / 12\).
Antes de nada, sabemos que por LDGN \(\bar X \to^p \mu $ por lo que $2\bar X \to^d \theta\) (es consistente). Entonces, por TCL, tenemos que \(\sqrt n (\bar x - \theta/2) \to^d N(0,\theta^2/12)\) así que \(\sqrt n (2\bar x - \theta)\to^d N(0,\theta^2/3)\). Este el resultado teórico.

Si consideramos $n = 20,\theta=10$, tenemos que $\theta^2/3n = 100/60$ así que todos los valores de nuestra distribución estrán en el intervalo $[10 \pm 2.6] \sim [7.4,12.6]$.



\section{Bootstrap}

Our goal in bootstrapping will be to approximate the distribution of the estimator $T= T(x_1,\dots,x_n;F)$. The idea would be to take different samples from the distribution and then compute the 

Hence, we would like to approximate
\[
H_n(x) = P_F(T(X_1,\dots,X_n;F) \leq x)
\]

If we knew the distribution function $F$, we could generate samples and the generate the histogram of the distribution of $T$. However, in this case, $F$ is unknown. In bootstrap, we will make use of the empiric distribution $F_n$ that the initial sample that we have provides. Hence, we can approximate:
\[
\hat{H_n}(x) = P_{F_n}(T(X^*_1,\dots,X^*_n,F_n)\leq x)
\]
This is called *ideal bootstrap*.

We can generate new samples by extracting elements from the original sample **with replacement**. We get new samples where all the elements belong to the initial sample but some elements can be repeated. This way, we can approximate the value of $\hat{H_n}(x)$. We compute for each generated sample $T^{*(b)} = T(X_1^{*b},\dots, X_n^{*b};F_n)$ and, lastly:
\[
\hat{H_n}(x) \approx \frac{1}{B} \sum_{b=1}^B I_{T^{*b} \leq x}
\]

Recall that we are approximating $\hat{H_n}$. We have then two approximations
\[
H_n(x) \approx \hat{H_n}(x) \approx \tilde{H_B}(x)
\]
the first one is approximating $F$ by $F_n$. Then, we approximate $H_n$ by $\hat{H_n}$. The first one is problematic since it requires a large amount of samples and **regularidad (translate)**.

\subsection{ Variance bootstrapping}

We can also estimate the variance of an estimator $\theta$: $Var_F(\hat \theta)$. The process is approximately the same, we firstly compute the ideal bootstrap and then the approximation based in $B$ re-samples is:
\[
Var_{F_n}(\hat{\theta^*}) \approx \frac{1}{B-1} \sum_{j=1}^B(\hat{\theta^*_j} - \bar{\theta^*})^2,
\]
where $\hat \theta^*_j$ is the estimator for the re-sample $j$.
