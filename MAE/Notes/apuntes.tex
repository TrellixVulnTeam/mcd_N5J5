\[
P(|\hat \theta_n - \theta | > \epsilon) = P(|\hat \theta_n - \theta |^2 > \epsilon^2) \leq \frac{E|\hat \theta - \theta|^2}{\epsilon^2} = \frac{sesgo^2(\hat\theta) + Var(\hat \theta)}{\epsilon^2} 
\]



Si tenemos 
\[
\sqrt{n} (\bar{X_n} - \mu)\to^d N(0,\sigma^2),
\]
 como \(\sqrt{n} \to \infty\) y la diferencia tiende a cero, nos está indicando que la velocidad con la que \(\bar{X_n}\) se acerca a \(\mu\) es la misma con la que \(\frac{1}{\sqrt{n}}\) va a cero




Sean \(X_1,\dots,X_n \sim U(0,\theta)\). Sabemos que \(\mu = \theta/2$ y que $\sigma^2 = \theta^2 / 12\).
Antes de nada, sabemos que por LDGN \(\bar X \to^p \mu $ por lo que $2\bar X \to^d \theta\) (es consistente). Entonces, por TCL, tenemos que \(\sqrt n (\bar x - \theta/2) \to^d N(0,\theta^2/12)\) así que \(\sqrt n (2\bar x - \theta)\to^d N(0,\theta^2/3)\). Este el resultado teórico.

Si consideramos $n = 20,\theta=10$, tenemos que $\theta^2/3n = 100/60$ así que todos los valores de nuestra distribución estrán en el intervalo $[10 \pm 2.6] \sim [7.4,12.6]$.



\section{Bootstrap}

Our goal in bootstrapping will be to approximate the distribution of the estimator $T= T(x_1,\dots,x_n;F)$. The idea would be to take different samples from the distribution and then compute the 

Hence, we would like to approximate
\[
H_n(x) = P_F(T(X_1,\dots,X_n;F) \leq x)
\]

If we knew the distribution function $F$, we could generate samples and the generate the histogram of the distribution of $T$. However, in this case, $F$ is unknown. In bootstrap, we will make use of the empiric distribution $F_n$ that the initial sample that we have provides. Hence, we can approximate:
\[
\hat{H_n}(x) = P_{F_n}(T(X^*_1,\dots,X^*_n,F_n)\leq x)
\]
This is called *ideal bootstrap*.

We can generate new samples by extracting elements from the original sample **with replacement**. We get new samples where all the elements belong to the initial sample but some elements can be repeated. This way, we can approximate the value of $\hat{H_n}(x)$. We compute for each generated sample $T^{*(b)} = T(X_1^{*b},\dots, X_n^{*b};F_n)$ and, lastly:
\[
\hat{H_n}(x) \approx \frac{1}{B} \sum_{b=1}^B I_{T^{*b} \leq x}
\]

Recall that we are approximating $\hat{H_n}$. We have then two approximations
\[
H_n(x) \approx \hat{H_n}(x) \approx \tilde{H_B}(x)
\]
the first one is approximating $F$ by $F_n$. Then, we approximate $H_n$ by $\hat{H_n}$. The first one is problematic since it requires a large amount of samples and **regularidad (translate)**.

\subsection{ Variance bootstrapping}

We can also estimate the variance of an estimator $\theta$: $Var_F(\hat \theta)$. The process is approximately the same, we firstly compute the ideal bootstrap and then the approximation based in $B$ re-samples is:
\[
Var_{F_n}(\hat{\theta^*}) \approx \frac{1}{B-1} \sum_{j=1}^B(\hat{\theta^*_j} - \bar{\theta^*})^2,
\]
where $\hat \theta^*_j$ is the estimator for the re-sample $j$.


\section{ PDF Estimation}


Let $X_1,\dots,X_n$ be an iid sample from the distribution $F$ with p.d.f. $f$. We woud like to estimate $f$ making no previous assumptions, just making use of the data. Our goal will be to compute an estimator $\hat f$ such that $\hat f \approx f$. Let $h \approx 0$, then
\[
P(x-h \leq X \leq x+h) = \int_{x-h}^{x+h} f(t) dt \approx 2hf(x)
\]
that means that we can approximate $f(x)$ as follows:
\[
f(x) \approx \frac{1}{2h} P(x-h \leq X \leq x+h).
\]
If we replace the probability by the proportion we obtain an p.d.f. estimator:
\[
\hat{f}(x) = \frac{1}{2h} \frac{\#\{i : |x-X_i|< h\}}{n} = \frac{1}{2hn}\#\{i : \frac{|x-X_i|}{h} \leq 1\}.
\]
If $K(x) = \frac{1}{2} \mathbb I_{\{|x| \leq 1\}}$, where $\mathbb I_A$ is the indicator function over $A$, then
\[
\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x-x_i}{h}\right)
\]

\subsection{ Estimating the kernel as a convolution}

Let $U = Y+Z$ where:
\begin{enumerate}
\item $Y \sim F_n$ where $F_n$ is the empiric distribution function
\item $Z$'s density function is $K_h(x) = h^{-1}K(x/h)$.
\item $Y$ and $Z$ are independent.
\end{enumerate}

that is, making a small perturbation on the empiric distribution. We can understand the kernel estimator as a slightly modified(smoothed) empiric distribution so that it is continuous.

\begin{nprop}
In the previous conditions, the p.d.f. of $U$ is the kernel estimator $\hat f$.
\end{nprop}
\begin{proof}
Firstly, we recall that
\[
F_u(x) = P(u \leq x) = P(y+z \leq x)
\]
now, using the total probability formula, we obtain
\[
P(y+z \leq x) = \frac{1}{n}\sum_{i=1}^n P(y+z \leq x | y = x_i) = \frac{1}{n}\sum_{i=1}^n P(z \leq x - x_i)
\]
in the last step, we can \emph{forget} the conditional since $Z,Y$ are independent. Then, we can use the definition of the probability to obtain:
\[
\frac{1}{n}\sum_{i=1}^n P(z \leq x - x_i) = \frac{1}{nh} \sum_{i=1}^n \int_{-\infty}^{x-x_i} k(\frac{u}{h})du
\]
We can now derivate the distribution function to obtain
\[
f_u(x) = F_u'(x) = \frac{1}{nh} = \sum_{i=1}^n k(\frac{x-x_i}{h}) = \hat{f}(x)
\]
\end{proof}

Using this, we can have an \emph{algorithm} to obtain an iid random variable with p.d.f. $\hat f$ following the next steps:
\begin{enumerate}
\item Pick a random element with the same probability in the set $X_1,\dots,X_n$. Let the result be $X^*$.
\item Simulate items to obtain $Z$ with distribution $K_h$
\item Compute $U = X^* +Z$
\end{enumerate}


\subsection{ Integrating the MSE }

In general, we use the bias and the variance of an estimator to determine its \emph{goodness}. Recall that
\begin{enumerate}
\item The bias of $\hat f (x)$ is $E[\hat f (x)] - f(x)$
\item The variance of $\hat f(x)$ is $E[(\hat f(x) - E[\hat f (x)])^2]$
\item The MSE $E[\hat f(x) - f(x)]^2$ verifies:
\[
MSE = Bias^2[\hat f(x)] + Var[\hat f(x)].
\]
\end{enumerate}

We would like to determine the MSE in terms of the hyperparameters $h,n$ and the smoothness of $f$. If we use higer values for $h$, then we would have a high bias since we would be forgetting the data. If we use very small values for $h$, our estimator would be very colse to the data, we would have a high variance and very small bias. 

\subsubsection{ Other criteria}

There are also other criteria to integrate the MSE. For instance, considering the $||\dot||_p$ in $L_p$, we can see that the ECMI is the expectation of the $L_2$ distance between $f$ and $\hat f$ squared, that is:
\[
ECMI(\hat f) = E[||f-\hat f||_2^2]
\]
It is easily shown seeing that:
\[
E(||f-\hat f||^2_2) = E[ \int |f(x) - \hat f(x)|^2 dx]
\]
and, using Fubini's theorem, we obtain:
\[
E[ \int |f(x) - \hat f(x)|^2 dx] = \int E[(f(x) - \hat f(x))^2] dx = \int MSE(x) dx
\]

\section{ Bias and Variance approximations}

We will make the following assumptions for this section:
\begin{itemize}
\item $K$ is a symmetric function with $\int K(u)du = 1$, $\int uK(u)du = 0$.
\item $\sigma_K^2 = \int u^2 K(u) du < \infty$ and $d_k = ||K||_2^2 = \int K(u)^2 du < \infty$.
\item $f$ is twice \emph{derivable} with continuous derivative.
\end{itemize}

\begin{nprop}
The bias... (diapositiva 21/37)
\end{nprop}
\begin{proof}

Then, the approximation of the bias can be obtained as follows:
\[
E[\hat f(x)] = E \left[ \frac{1}{nh} = \sum K \left( \frac{x-x_i}{h}\right)\right] = \frac{1}{n} E[K \left( \frac{x-x_i}{h}\right] = \frac{1}{n}\int K \left( \frac{x-t}{h} f(t) dt\right)
\]
we apply a \emph{variable change} \(w = \frac{x-t}{h}\), \(dw = \frac{-dt}{h}\), \(t = x -wh\), we obtain
\[
 \frac{1}{n}\int K \left( \frac{x-t}{h} f(t) dt\right) = \int K(w) f(x-wh) dw
\]
and, applying Taylor's theorem (forgetting the terms that have $h^3$ and so on since $h$ will be very small),
\[
\int K(w) f(x-wh) dw = \int K(w)\left[ f(x) - whf'(x) + \frac{w^2 h^2}{2}f''(x)'\right]dw = f(x) + \frac{h^2}{2}f''(x) \int w^2 K(w) dw
\]
where the last integral equals $\sigma_k^2$, so
\[
Bias(\hat f(x)) = \frac{h^2}{2}f''(x) \sigma_k^2.
\]
As we can see, the bias of $\hat f$ depends highly on $h$.

\end{proof}

\begin{nprop}
The variance..
\end{nprop}

\begin{proof}
We can see that
\[
Var[\hat f(x)] = Var\left[ \frac{1}{nh} = \sum K \left( \frac{x-x_i}{h}\right)\right] = \frac{1}{h^2n} Var\left[ K\left(K(\frac{x-x_i}{h})\right)\right]
 =  \frac{1}{h^2n} \left[ E\left[ K^2(\frac{x-x_i}{h})\right] - E^2 \left[ K(\frac{x-x_i}{h})\right]\right]
 \]
 which is approximately 
 \[
 \frac{1}{h^2n}E\left[K^2(\frac{x-x_i}{h})\right] = \frac{1}{nh^2}\int K^2(\frac{x-t}{h}) f(t)dt
 \]
 and, applying the same \emph{variable change}, and Taylor's theorem:
 \[
  = \frac{1}{nh^2} \int K^2(w) f(x-hw)dw \approx \frac{1}{nh} \int K^2(w)\left[ f(x) - hwf'(x) + \frac{h^2w^2}{2}f''(x)\right]dw
 \]
 Since we are dividing by $nh$, we can forget the term $f'(x)$ term, so we finally obtain:
 \[
 Var[\hat f] = \frac{1}{nh} f(x) d_k
 \]
\end{proof}

Using both previous propositions, we obtain:
\begin{nprop}
The approximation of the MSEI is:
\[
MSEI(\hat f) \approx \frac{h^4}{4}\sigma_k^4 \int f''(x)^2 dx  + \frac{d_K}{nh} = \frac{h^4}{4}\sigma_K ||f''||_2^2 + \frac{||K||^2_2}{nh}
\]
\end{nprop}

The idea now is to optimize this function with respect $h$. We have to derivate this function respect to $h$ and make it equal to $0$, that is:
\[
\frac{4h^3}{4}\sigma_K^4 ||f''||_2^2 = \frac{||K||_2^2}{nh^2}
\]
and the optimal value is
\[
h^* = c n^{-1/5} \to 0
\]
which implies
\[
nh^* = cn^{4/5}\to \infty
\]

The conclusions are:
\begin{itemize}
\item $h^*$ has to go to zero but slowly
\item The speed at which $h$ goes to zero in this case is $n^{-4/5}$, which is pretty slower than just $n$.
\end{itemize}



\subsubsection{Optimal kernel}
What is more, when computing the IMSE, the factor that depends on \(K\) is \(\sigma(K) = \sigma_K^{4/5}||K||_2^{8/5}\) . It can be proved that if \(K_h(\cdot) = \frac{1}{h} K(\frac{\cdot}{h})\). 

To optimize the kernel, we have the following variational problem:
\[
\min ||K||^2_2 \quad s.t.
\]

The solution to this problem is the Epanechnikov
\[
K^*(u) = \frac{3}{4\sqrt{5}} \left( 1 - \frac{t^2}{5}\right) \quad \text{ if } -\sqrt{5} \leq u \leq \sqrt{5}
\]
The choice of the kernel does not hardly affect the value of the IMSE.


\subsubsection{Choosing the smoothing parametern}
To select the optimal smoothing parameter there are many methods:

\begin{enumerate}
\item Plug in:
  \begin{itemize}
  \item Suppose that $f \approx N(\mu,\sigma)$
    \item Non parametric methods
  \end{itemize}
  \item Cross validation
\end{enumerate}

\textbf{Plug-in supposing Normal.-}

We assume that
\[
f(x) = \frac{1}{\sigma \sqrt{2\pi}}\exp...
\]
So, we substitute \(\norm{f''}_2^2\) in the expression of the optimal parameter \(h^*\) and , since we already know that the value of the norm, we obtain

\[
h^* = \sigma(4/(3n))^{1/5} \approx \sigma(1.0456)n^{-1/5}
\]
So, since the number is approximately \(1\), it is enough to estimate \(\sigma\) to estimate \(h^*\). Some literature (\emph{Silverman}) proposes to use \(\hat \sigma = \min \{ s,\hat\sigma_{ri}\}\) where \(\hat \sigma_{ri}\) is the interquartilic range (estandardized so that it converges to \(\sigma\)).


\textbf{Non-parametric plug-in.-}

We fix a preliminar smoothing parameter $g$, using for instance the previous approximation. This way, we obtain an auxiliar estimator \(\hat f_g (x) \). Using this estimator, we now estimate \[
\hat{\norm{f''}_2^2} = \int \hat f_g ''(x)^2dx
  \]


  \textbf{ Cross Validation }

  The intuitive idea under cross validation is to divide the population in two splits and we use one of them to obtain information of how good is the estimator computed with the first one. We can then compute
  \[
MSEI(\hat f) = E \left[ \int \hat f (x;h)^2 dx\right] - 2E\left[ \int \hat f (x;h)f(x) dx \right] + \int f(x)^2 dx
  \]
  Again, we would like to find \(h^*\) such that this IMSE is small. As we can see, the last term only depends on the \emph{true} density function. For each \(h\), our sample is fixed, so the expectation is reduced to a single term and we would like to minimize
  \[
  C(h) =   \int \hat f (x;h)^2 dx - \frac{2}{n} \sum \hat f_{(-i)} (X_i;h)
  \]
  where \(f_{(-i)}\) is the estimator computed with all the observations except \(X_i\). The last term comes from the following reasoning. Let \(x_1,\dots,x_n \to \hat f\), and let us have another point \(x_{n+1}\). Then, we can see that:
  \[
  \int \hat f (x;h) f(x) dx = E \left[ \hat f (x_{n+1};h) | x_1,\dots x_n \right]
  \]
  If we take expectation in both sides and use the known formula, we obtain:
  \[
  E\left[ \int \hat f (x;h) f(x) dx\right] = E \left[ \hat f (x_{n+1};h) \right]
  \]


\subsection{Multivarian density estimation}

Consider \(d\) dimensions, we have that
\[
\hat f(x) = \frac{1}{n|H|} \sum \tilde{K}(H^{-1}(x-X_i)), \quad x =(x_1,\dots,x_d) \in \R^d
\]
where \(\tilde K\) is a multivariant density, \(\Sigma \in \mathcal M_{d\times d}\) positive definited (?), \(H = \Sigma^{1/2}\) and \(|H|\) is the determinant of \(H\). Note that, if \(\Sigma = CDC'\) where \(D\) is a diagonal matrix, we define \(\Sigma^{1/2} = CD^{1/2}C'\) where \(D^{1/2}\) has the squared roots of the elements. The problem of this definition is that there is a wide range of parameters that we have to fix.

We approach this problem by simplyfying the expression:
\begin{itemize}
\item We consider \(\tilde K\) as the product of identic unidimensional kernels:
\[
\tilde K(x_1,\dots,x_d) = K(x_1)\dots K(x_d)
\]
\item We consider \(H\) to be diagonal and each element in \(H\) to be the same, that is: \(H = h I_d\).
\end{itemize}

With this considerations, we obtain
\[
\hat f(x) = \frac{1}{nh^d} \sum_{i=1}^n \prod_{j=1}^d K \left(\frac{x_j - X_{i,j}}{h}), \quad x=(x_1,\dots,x_d) \in \R^d
\]

\subsubsection{The curse of dimensionality}

Unless we have huge amounts of data, it is quite hard to find data in many zones of the sample space, so the properties of our estimators are worsen.

For instance, it can be shown that in dimension \(d\),
\[
ECMI^* \approx O(n^{\frac{-4}{4+d}})
\]


\section{Regression}



The general goal in a regression problem is, given a random variable \(Y\) and a vector of random regressor variables \(X = \left(X_1,\dots,X_p\right)\), study the relation between \(X\) and \(Y\). Remark that \(Y\) can also be a random vector and in this case we are in the case of multivariant regression.

\begin{ndef}
In the previous conditions, we call the regression function to:
\[
m(X) = E\left[Y|X\right], \quad m(x) = E\left[Y|X = x\right].
\]
\end{ndef}

It is known that \(m(X)\) is the best prediction of \(Y\) from \(X\) (using the MSE). Our goal is to estimate \(\hat m(x)\).

Usually, regression models are expresed as follows:
\[
Y = m(X) + \epsilon,
\]
where \(E(\epsilon | X) = 0\) or, equivalently, \(m(X) = E\[Y|X\]\). That can be proved the following way:
\[
E[Y|X] = E \left[ m(x) + \epsilon | x\right] = m(x) + E\left[ \epsilon | x\right] = m(x)
\]
because the expectation \(E \left[\epsilon | x\right] = 0\). These hypothesis provide the following statements:

\begin{itemize}
\item \(E(\epsilon) = E(E(\epsilon | X)) = E(0) = 0\)
\item \(Var(\epsion = 0)\) using the formula of the total variance.
\item \(E(\epsilon X) = E\left[ E(\epsilon X | X)\right] = E\left[ E(\epsilon | X)\right] = 0\).
\item Lastly, the last item implies that \(Cov(\epsilon,X) = E(\epsilonX) - E(\epsilon)E(X) = 0 - 0\cdot E(X)\).
\end{itemize}

There are many different possible models:
\begin{enumerate}
\item Linear regression, where
\[
m(X) = m(X_1,\dots,X_p) = \beta_0 + \beta_1X_1 + \dots + \beta_p X_p
\]

\item Non parametric models, that assume continuity or smoothness conditions for \(m(x)\).
\end{enumerate}

\subsection{Non parametric regression. Nadaraya-Watson Estimator}

Let us assume that the vector \(X,Y\) has a joint p.d.f \(f(x,y)\) and the marginal pdf of \(X\) is \(g(x)\), then:
\[
m(x) = E\left[Y| X = x\right] = \int y f(y|x) dy = \frac{\int y f(x,y) dy}{g(x)}.
\]
The main idea is to use a kernel estimator in the numerator and in the denominator to replace the densities that appear in the last expression.
\[
\hat g(x) = \frac{1}{nh} \sum K\left( \frac{x-X_i}{h}\right)
\]
and, hence,
\[

\]

then, the estimator is:
\[
\hat m(x) = \frac{\sum  K\left( \frac{x-X_i}{h}\right Y_i)}{\sum }{}}
\]

\begin{proof}
\begin{align*}
\int y \hat f(x,y) dy & = \frac{1}{nh^2} \sum K \left(\frac{x-X_i}{h}\right) \int y  K \left(\frac{x-X_i}{h}\right) dy \\
& \stackrel{=}{(1)} \frac{1}{nh} \sum K \left(\frac{x-X_i}{h}\right) \underbrace{\int(uh+Y_i) K(u)}_{(2)} du
\end{align*}
where, in \(1\) we have applied the change of variables \(u = \frac{y-y_i}{h}\) and in \((2)\) we have used that \(\int uk(u)du = 0\) and that \(\int k(u)du =1\). To end this proof, we insert this term in the expression of \(\hat m(x)\) and we finish the proof
\end{proof}

We can change the way of expressing \(\hat m(x)\). If we name
\[
w_i(x) = \frac{ K \left(\frac{x-X_i}{h}\right)}{\sum  K \left(\frac{x-X_i}{h}\right)},
\]
then \(\hat m(x) = \sum w_i(x) y_i\).

\begin{remark}
The estimator is an average of \(Y_i\) locally weighted such that, to estimate \(m(x)\), we use only the \(Y_i\) such that \(X_i \approx x\).
\end{remark}

A natural question is to consider the extreme cases of the smoothing parameter \(h\) of the kernel.
\begin{itemize}
\item If \(h \to \infty\), we have that
\[
w_i(x) \to_{h \to \infty} \frac{K(0)}{nK(0)} \frac{1}{n}
\]
and, then,
\[
\hat m(x) = \sum \frac{1}{n} Y_i = \hat Y
\]

\item If \(h \to 0\), since \( K \left(\frac{x-X_i}{h}\right) \to 0\) when \(j \neq i\) and \(K(0)\) if \(i = j\), which implies
\[
\hat m(x_i) \to_{h \to 0} Y_i
\]
\end{itemize}

We can also make an interpretation using the mean squared error. The NW estimator computed in \(x\) is the ... complete (7/54).


We can also approximate the bias and the variance from the NW estimator:

\begin{nprop}
  \[
    \int Var [\hat m(x)] dx \approx \frac{\sigma^2 ||K||_2^2}{nh} \int \frac{dx}{h(x)}, \quad nh >>> 0
  \]
  and the bias term:
  \[
    \int (E(\hat m(x) - m(x))^{2})dx \approx \frac{h^{4}}{4}\sigma_{K}^{4} \int \left( m''(x) COMPLETE\right)
  \]

\end{nprop}

\subsubsection{Penalized MSE}

In this case, the estimatos are functions that minimize \(\phi(\lambda)\) for \(\lambda > 0\) and
\[
\phi(\lambda):= \sum_{i}(Y_{i} - m(x_{i}))^{2} + \lambda \int_{a}^{b} m''(x)^{2} dx
\]

The first term measures how our model fits the data. The second term controls the smoothness of our estimator. We can think of it as the regularization term used frequently in machine learning.

We have to consider the following extreme situations:

\begin{enumerate}
  \item \[
        \lim_{\lambda \to \infty} \hat m(x) = \hat \beta_{0} + \hat \beta_{1}x
        \]
  \item
        \[
        \lim_{\lambda \to 0} \hat m(x) = interpolator
        \]
\end{enumerate}

\begin{remark}
The solution of this problem, which is a tradeoff between fitting our data and being smooth, is a spline.
\end{remark}

\subsection{Multiple linear regression}

Recall that the most ocmmon regression model is:
\[
Y = \beta_{0} + \sum_{j}^{p} \beta_{j}X_{j} + \epsilon
\]
where \(E(\epsilon | X_{1},\dots,X_{p}) = 0\) and its variance is \(\sigma^2\). We can assume that, for many inferences (intervals, contrasts,...) \(\epsilon|(X_{1},\dots,X_{p})\) follows a gaussian distribution.

Each observation from the training sample follows the model:
\[
y_{i} = \beta_{0} + \sum_{j}^{p} \beta_{j}x_{i,j} + \epsilon_{i}, \quad i = 1,\dots,n
\]
where, again, \(E(\epsilon_{i}|x_{i}) = 0\) and \(Var(\epsilon_{i}|x_{i}) = \sigma^{2}\).


...................
.........................
..............................
..........................................

\subsubsection{Prediction errors}

Suppose that we have a real model, predicting some data. Let \(Y = (Y_{1},\dots,Y_{n})\) be the labels of the training data, \(\tilde Y = (\tilde Y_{1},\dots, \tilde Y_{n})\) the labels of the test data. We hipothesize that the vectors \(Y,\tilde Y\) are independent. We assume that \(E(Y) = E(\tilde Y) = \mu = (\mu_{1},\dots, \mu_{n})\) and \(\Sigma\) be the covariance matrix.

We fit a regression model \(Y = X\Beta + \epsilon \iif Y = \mu + \epsilon\) where \(\mu \in V = \{X\Beta \ : \ \Beta \in \mathbb R^{p}\}\). We already know that:

\begin{itemize}
  \item The MSE model is $\hat \Beta = (X'X)^{-1}X'Y$
        \item ... D(44/57)
\end{itemize}


\begin{ndef}
  We call the training error:
  \[
E_{\text{train}}E \left[\sum_{i=1}^{n}(Y_{i} - x_{i}'\hat \Beta)^{2}]
\]
and we can define the test error the same way using test labels.
\end{ndef}

\begin{nprop}
  The test error can be expressed as:
  \[
    E_{\text{test}} = E_{\text{train}} + 2E(\norm{C}^2)
    \]

  \end{nprop}
  \begin{proof}
    Recall that:
  \[
E_{\text{test}} = E(\norm{A}^{2}) + E(\norm{B}^{2}) + E(\norm{C}^{2}) + 2E(A'B) + 2E(A'C) + 2E(B'C)
\]
and, since \(E(A'B) = E(A'C) = E(B'C) = 0\) and using that \(E(\norm{B}^{2})\) and \(E(\norm{C}^{2})\) are identical and iid, we obtain the final result.
\end{proof}

Having the previous proposition, we would like to calculate the last term of the error:
\[
E(\norm{C}^{2}) = E[(\hat \Beta - \Beta)' X'X(\hat \Beta - \Beta^{*})]
\]

We can use the following \emph{trick}:
\[
E(\norm{C}^{2}) = E[(\hat \Beta - \Beta)' X'X(\hat \Beta - \Beta^{*})] = E(\norm{C}^{2}) = E[tr((\hat \Beta - \Beta)' X'X(\hat \Beta - \Beta^{*}))] = E(\norm{C}^{2}) = E[tr(XX'(\hat \Beta - \Beta^{*})(\hat \Beta - \Beta^{*})')] = tr(E\left[ (X'X)(X'X)^{-1} X'\Sigma X (X'X)^{-1}\right])
\]
and, recalling that \(X(X'X)^{-1}X' = H\), we obtain that
\[
 tr(E\left[ (X'X)(X'X)^{-1} X'\Sigma X (X'X)^{-1}\right]) = tr(\Sigma H).
\]

We have just proved that
\[
E_{\text{test}} = E_{\text{train}} + 2tr(\Sigma H).
\]
Furthermore, recalling that
\[
Cov(Y,\hat Y) = Cov(Y,HY) = \Sigma H
\]
we obtain that
\[
E_{\text{test}} = E_{\text{train}} + 2 \sum_{i=1}^{n}Cov(Y_{i},\hat Y_{i}).
\]

An interpretation for this is that, if \(Y_{i} \approx \hat Y_{i}\), then we are being pretty optimistic.

\textbf{Case: Independence and homocedastic XDXDXDXXDXDXX}

Consider that
\[
\begin{pmatrix} \sigma^{2} & 0 \\ 0 & \sigma^{2} \end{pmatrix} = \sigma^{2} I
\]
then,
\[
  tr(\Sigma H) = \sigma^{2} tr(H) = \sigma^{2}p
\]

\[
tr(X(X'X^{-1}X')) = p
\]

Hence,
\[
E_{\text{test}} = E_{\text{train}} + 2p\sigma^{2}
\]


\textbf{Case: Independence and heterocedastic XDXDXD}

In this case,
\[
\begin{pmatrix} \sigma_{1}^{2} & 0 \\ 0 & \sigma_{n}^{2} \end{pmatrix} = \sigma^{2} I
\]

Hence,
\[
E_{\text{test}} = E_{\text{train}} + 2\sigma_{i}^{n}\sigma_{i}^{2} h_{ii}
\]
