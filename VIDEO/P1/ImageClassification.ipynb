{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[UAM-APPSIV] Image Classification (1).ipynb","provenance":[{"file_id":"1YKVUF1QM5fsud81-mJ0KzAQNC7VuPgRM","timestamp":1602835967074},{"file_id":"158w7iWehDsxmhf9cVwYwPfTzO6x_mqKs","timestamp":1598604890788},{"file_id":"1P4k-rvT9eBFcoOCskAoFDSExUwwuZsxq","timestamp":1586961498321},{"file_id":"1l_gR6zIqWwoXxuZvuvKLceYeXYLRz2Kz","timestamp":1586013805256},{"file_id":"1tcYFztTo99cU74XZDv7Nf0DQ84PhXFRV","timestamp":1585916751831}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GtiQjkq8PnI6"},"source":["# Image Classification with PyTorch: Simple CNN \n","\n","[Pablo Carballeira] (http://dymas.ii.uam.es/webvpu/gti/user/186/), Escuela Politecnica Superior, Universidad Autónoma de Madrid.\n","\n","Parts of this code have been adapted from then work of Kevin McGuinness (http://www.eeng.dcu.ie/~mcguinne/), School of Electronic Engineering, Dublin City University, and the work of Ben Trevett (https://github.com/bentrevett), Heriot-Watt University\n","\n","You can find documentation about working in Colab here (https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","\n","---\n","\n","In this lab assignment you will learn how to use the [PyTorch](https://pytorch.org/) deep learning framework to create and train a simple deep learning model for handwritten digit classification. You will also learn about data loaders and datasets in PyTorch and how to track model training progress.\n"]},{"cell_type":"markdown","metadata":{"id":"FGmS80CT8aCw"},"source":["## Instructions\n","\n","Anywhere you see a **???** in the code below, fill in in with the correct code."]},{"cell_type":"markdown","metadata":{"id":"Ywspo1LiklkS"},"source":["# Import packages\n","\n","Find the PyTorch docs at https://pytorch.org/docs/stable/index.html \n","\n","Tutorials: https://pytorch.org/tutorials/"]},{"cell_type":"code","metadata":{"id":"V8TdKha5hoCs"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","\n","from torch.utils.data import DataLoader\n","# import torch.utils.data as data\n","\n","import torchvision.datasets as datasets\n","from torchvision.datasets import MNIST\n","\n","from sklearn import decomposition\n","from sklearn import manifold\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","import copy\n","import random\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MGvs0jJ0JgA"},"source":["# Enable GPU acceleration\n","\n","Open to the Edit menu and select *Notebook settings* and check that *GPU* is selected under hardware accelerator.\n"]},{"cell_type":"code","metadata":{"id":"eiSDmqiA2es2","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1602836655947,"user_tz":-120,"elapsed":5652,"user":{"displayName":"Pablo Carballeira López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUytUgwDP68UTBhS9F6uxQLZPT1jAcn0TpTdJ-Jw=s64","userId":"08621655621235223940"}},"outputId":"e9751eb4-a852-487f-9c9b-20460edb5ade"},"source":["# make sure to enable GPU acceleration!\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1FcvzjWq-0xs"},"source":["We set the random seed so all of our experiments can be reproduced."]},{"cell_type":"code","metadata":{"id":"9bexQC8PaL9w"},"source":["# Set random seed for reproducability\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","\n","random.seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTq9KUW_7dct"},"source":["# Specify a model architecture\n","\n","We will use this simple CNN with two conv layers, one pooling layer, two fully connected, and some dropout.\n","\n","![Model Architecture](http://www.eeng.dcu.ie/~mcguinne/media/cnn.png)"]},{"cell_type":"code","metadata":{"id":"KAaUNorqjKO_"},"source":["class SimpleCNN(nn.Module):\n","\n","    def __init__(self, num_channels=1, num_classes=10):\n","        super(SimpleCNN, self).__init__()\n","\n","        # create the first convolution layer (bottom of the diagram)\n","        self.conv1 = nn.Conv2d(num_channels, 32, 3, stride=1, padding=1)\n","\n","        # create the second convolution layer\n","        self.conv2 = ???\n","\n","        # create the 2x2 max pooling layer\n","        self.pool1 = ???\n","\n","        # create the dropout operation with p = 0.25\n","        self.drop1 = ???\n","        \n","        # create a fully connected layer mapping the output of the pooling layer to 128 dimensions. \n","        # Use nn.Linear \n","        self.fc1 = ???\n","        \n","        # create the second dropout operation with p = 0.5\n","        self.drop2 = ???\n","        \n","        # create the final fully connected layer mapping 128 dimensions to 10 classes\n","        self.fc2 = ???\n","        \n","    def forward(self, X):\n","\n","        # apply conv1 and a relu\n","        X = F.relu(self.conv1(X))\n","\n","        # apply conv2 and a relu\n","        X = ???\n","        \n","        # apply the pooling layer and dropout with p=0.25\n","        X = self.pool1(X)\n","        X = self.drop1(X)\n","\n","        # flatten the spatial features. You can use the reshape() function for this\n","        X = ???\n","        \n","        # apply the first fully connected layer and a relu\n","        X = ???\n","        \n","        # apply dropout with p = 0.5\n","        X = ???\n","        \n","        interm_features = X # we will use this output for an intermedeate representation of the network \n","\n","        # apply the final fully connected layer\n","        X = ???\n","        \n","        # return the logits (pre-softmax activations)\n","        return X, interm_features "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMahrmY05F01"},"source":["The code below is used to check that the model architecture is defined correctly. The code should not return any error, and the output should be the following: \n","\n","```\n","Output size:  torch.Size([1, 10])\n","Output: tensor([[ 0.0091,  0.0461, -0.0732, -0.1247,  0.1205, -0.1493, -0.1618,  0.0994, -0.0953,  0.0748]], grad_fn=<AddmmBackward0>)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"6Skxdd0E6j3S"},"source":["SEED = 1234\n","torch.manual_seed(SEED)\n","\n","dummy_input = torch.rand(1,1,28,28)\n","\n","model = SimpleCNN()\n","output = model(dummy_input)\n","\n","print('Output size: ',format(output[0].shape))\n","print('Output:')\n","print(output[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UrynDn0t7rLY"},"source":["# Load the datasets\n","\n","Here, we will download and prepare the MNIST dataset. For that, we need to specify a **transform** to convert images to torch tensors.\n","\n","We are adding a **normalization** transform here too so that the images have mean zero and unit variance. This is optional. For some problems (models/datasets) proper normalization is important for performance. For others (e.g. models with batch normalization early on), the importance of normalization is less.\n","\n","PyTorch comes with a built-in dataset class for the MNIST digit classification task in the ``torchvision`` package. It also has built-in for other common datasets and tasks like CIFAR-10 and ImageNet. See: https://pytorch.org/docs/stable/torchvision/datasets.html\n"]},{"cell_type":"code","metadata":{"id":"oe6InO7QnZca"},"source":["# transform for the training data\n","# normalization uses mean and std values of the training data\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.1307], [0.3081]) # normalization coefficients are precomputed\n","])\n","\n","# use the same transform for the validation data\n","valid_transform = ???\n","\n","# load train data, downloading if needed\n","train_set = MNIST('./data/mnist', train=True, download=True, \n","                  transform=train_transform)\n","# load validation data, downloading if needed\n","valid_set = ???\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Sl9tT_Jnym0"},"source":["print(f'train set is', ' x '.join(str(x) for x in train_set.data.shape))\n","print(f'valid set is', ' x '.join(str(x) for x in valid_set.data.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ykAVQWwrRLU3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G3zD2yzfU3dC"},"source":["## Some notes about the training data\n","\n","There are two subsets of the data being used here:\n","\n","- **Training data** Data that is used to optimize model parameters\n","- **Validation data** Data that is used for model selection (choosing hyperparameters).\n","\n","The data that we use to monitor the training progress is the validation data, since it can be used to tune the model architecture (number of layers, etc) and other hyperparameters.\n","\n","Usually, we would keep another separate **test set** for testing the final model in order to get an unbiased estimate of *out of sample* accuracy. Unfortunately, MNIST doesn't have a separate test set and it is common practice on this task to use the validation set both for validation and test. **Warning**: This is considered BAD PRACTICE in most situations!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9NDg-mJ23kg5"},"source":["# Preview the data\n","\n","Let's look at a sample of the training data. Here we'll use some indexing tricks (reshaping and permuting axes) to put the first 64 digits in the dataset into a grid."]},{"cell_type":"code","metadata":{"id":"x0uIuozf57GB"},"source":["# grab the first 64 samples from the training set\n","sample = train_set.data[:64]\n","\n","# sample shape is (64, 28, 28), but we would like a grid of 8x8 images, each of size 28x28.\n","# this would mean an image with width and height of 8*28. How do we go from a (64, 28, 28)\n","# image to one of size (8*28, 8*28)?\n","\n","# start by splitting the first dimension (64) int 8 x 8\n","sample = sample.reshape(8, 8, 28, 28)\n","\n","# the shape is now (8, 8, 28, 28). Next, move the second dimension to the third position\n","sample = sample.permute(0, 2, 1, 3)\n","\n","# the shape is now (8, 28, 8, 28). Finally, collapse the first-second and third-forth\n","# dimensions into one.\n","sample = sample.reshape(8*28, 8*28)\n","\n","# Voila! the shape is (8*28, 8*28). Note that we can't just reshape to this shape immediately,\n","# because the dimension permute in step two above is also needed.\n","\n","# Show the images\n","plt.figure(figsize=(10,10))\n","plt.imshow(sample)\n","plt.axis('off')\n","plt.title('First 64 MNIST digits in the training set')\n","plt.gray()\n","plt.show()\n","\n","print('Labels:', train_set.targets[:64].numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdExkrRS43Fo"},"source":["# Setup the data loaders\n","\n","PyTorch provides a [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class in `torch.utils.data` that can be used to manage loading data from datasets using multiple worker threads and packaging data into datasets. It also provides data **shuffling** and **sampling** strategies.\n","\n","Having multiple threads to load the data improves the performance when training larger models on large datasets since the CPU threads can be busy loading and transforming data while the GPU is doing forward and backward propagation.\n","\n","\n","**Note**: The `Dataset` and `DataLoader` classes are not mandatory in PyTorch; you can write your own data loading mechanisms. "]},{"cell_type":"code","metadata":{"id":"PIftYkluoDbF"},"source":["train_loader = DataLoader(train_set, batch_size=256, num_workers=0, shuffle=True)\n","\n","# create a similar data loader for the validation set, but turn off shuffling\n","# (it is unnecessary to shuffle validation data) and make the batch size 512\n","valid_loader = ???\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZlNXgHG6q-g"},"source":["# Instantiate the model\n","\n","Create an instance of the model and move it (memory and operations) to the CUDA device."]},{"cell_type":"code","metadata":{"id":"eU_kiKpypYu0"},"source":["model = SimpleCNN()\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7j7Ug2cwUQM"},"source":["Let's count the total number of parameters in our model."]},{"cell_type":"code","metadata":{"id":"wn-3_0WvRRdR"},"source":["def count_parameters(model):\n","    total = 0\n","    for p in model.parameters():\n","        \n","        # compute the total parameters in the layer\n","        # which is equal to the product of the dimensions\n","        # in p.shape (np.prod may be helpful)\n","        layer_count = ???\n","        total = ???\n","        \n","    return total\n","\n","\n","print(f'The model has {count_parameters(model)} parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vPrYMlDSEljV"},"source":["# Train the model\n","\n","We're almost ready to train the model!\n","\n","Each pass through the training loop is called an *epoch* (an epoch is when every training example has been seen once). \n","\n","It's a good idea to save a snapshot (or *checkpoint*) of the model and the optimizer after every epoch. Let's setup some helpers to do this"]},{"cell_type":"code","metadata":{"id":"QYk-zSLJRqx9"},"source":["# we will save checkpoints to the checkpoints folder. Create it.\n","!mkdir -p checkpoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vpx7HCnXRKoM"},"source":["def save_checkpoint(optimizer, model, epoch, filename):\n","    checkpoint_dict = {\n","        'optimizer': optimizer.state_dict(),\n","        'model': model.state_dict(),\n","        'epoch': epoch\n","    }\n","    torch.save(checkpoint_dict, filename)\n","\n","\n","def load_checkpoint(optimizer, model, filename):\n","    checkpoint_dict = torch.load(filename)\n","    epoch = checkpoint_dict['epoch']\n","    model.load_state_dict(checkpoint_dict['model'])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n","    return epoch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYLgPSzbS5KD"},"source":["Let's code up the training loop as a function. There are two parts to the loop:\n","- **Train phase**: where batches of data are loaded from the training set and the model parameters are optimized using backpropagation to compute gradients.\n","- **Validation phase**: where batches of data are loaded from the validation set and out of sample error is estimated using this data."]},{"cell_type":"code","metadata":{"id":"9QAuxIQvoSDV"},"source":["# create the criterion to optimize (the loss function). This is a multi-class classification \n","# problem, so categorical cross entropy is an appropriate loss.\n","# it also applies the softmax activation function\n","# criterion = ??\n","criterion = nn.CrossEntropyLoss()\n","\n","# create an SGD optimizer with learning rate 0.01, momentum 0.9, and nesterov momentum turned on\n","optimizer = ???\n","\n","def train_for_epoch():\n","\n","    # put model in train mode\n","    model.train()\n","\n","    # keep track of the training losses during the epoch\n","    train_losses = []\n","\n","    for batch, targets in train_loader:\n","        # Move the training data to the GPU\n","        batch = ???\n","        targets = ???\n","        \n","        # clear previous gradient computation\n","        optimizer.zero_grad()\n","\n","        # forward propagation\n","        predictions, interm_feats = ???\n","        \n","        # calculate the lossfor the batch\n","        loss = ???\n","        \n","        # backpropagate to compute gradients\n","        loss.backward()\n","\n","        # update model weights\n","        optimizer.step()\n","\n","        # update the array of batch losses\n","        train_losses.append(loss.item())\n","\n","    # calculate average training loss of the epch\n","    train_loss = ???\n","        \n","    return train_loss\n","\n","\n","def validate():\n","\n","    # put model in evaluation mode\n","    ???\n","\n","    # keep track of losses and predictions\n","    valid_losses = []\n","    y_pred = []\n","\n","    # We don't need gradients for validation, so wrap in \n","    # no_grad to save memory\n","    with torch.no_grad():\n","\n","        for batch, targets in valid_loader:\n","\n","            # Move the validation batch to the GPU\n","            batch = batch.to(device)\n","            targets = targets.to(device)\n","\n","            # forward propagation\n","            # predictions, interm_feats = ??? \n","            predictions, interm_feats = model(batch)\n","\n","            # calculate the loss\n","            loss = ???\n","\n","            # update running loss value\n","            valid_losses.append(loss.item())\n","\n","            # save predictions\n","            y_pred.extend(predictions.argmax(dim=1).cpu().numpy())\n","\n","    # compute the average validation loss\n","    valid_loss = ???\n","\n","    # Collect predictions into y_pred and ground truth into y_true\n","    y_pred = np.array(y_pred, dtype=np.float32)\n","    y_true = np.array(valid_set.targets, dtype=np.float32)\n","\n","    # Calculate accuracy as the average number of times y_true == y_pred\n","    accuracy = ???\n","\n","    return valid_loss, accuracy\n","\n","\n","def train(first_epoch, num_epochs):\n","    \n","    train_losses, valid_losses = [],  []\n","\n","    for epoch in range(first_epoch, first_epoch + num_epochs):\n","\n","        # training phase\n","        train_loss = train_for_epoch()\n","\n","        # validation phase\n","        valid_loss, valid_acc = validate()        \n","\n","        print(f'[{epoch:03d}] train loss: {train_loss:04f}  '\n","              f'val loss: {valid_loss:04f}  '\n","              f'val acc: {valid_acc*100:.4f}%')\n","        \n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","\n","        # Save a checkpoint\n","        checkpoint_filename = f'checkpoints/mnist-{epoch:03d}.pkl'\n","        save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n","    \n","    return train_losses, valid_losses"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVRUaMzEwLH3"},"source":["Train the model for 10 epochs using the functions we have defined\n","\n"]},{"cell_type":"code","metadata":{"id":"VDdFK6RDSZ1X"},"source":["train_losses, valid_losses = ??? "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Urau1xFIKtFY"},"source":["# Plot the learning curves\n","\n","Here we plot the learning curves after training. Note that you can also use [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) with PyTorch in Google Colab \n"]},{"cell_type":"code","metadata":{"id":"dzwmuvqrzXpE"},"source":["epochs = range(1, len(train_losses) + 1)\n","\n","plt.figure(figsize=(10,6))\n","# use matplotlib to plot the training and validation losses in a single graph,\n","# and label them accordingly\n","???\n","\n","plt.legend()\n","plt.title('Learning curves')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(epochs)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u1rSJSroPYaL"},"source":["# Error analysis\n","\n","Here we will take a look at the validation samples that the model got wrong. This is often a good idea when trying to improve a models, since it gives you some intuition about the kinds of examples the model finds difficult, which can lead to insights on how to improve models."]},{"cell_type":"markdown","metadata":{"id":"B-gBJCx29mjn"},"source":["We'll examine our model by: plotting a confusion matrix, seeing which incorrect examples our model was most confident about, view our model's learned representations in two dimensions with t-SNE, and take a look at the weights of our model.\n","\n","First, we'll define a function to fet the model's predictions across a dataset training/valid/test that alos provides other outputs"]},{"cell_type":"markdown","metadata":{"id":"zg0yyhz-t77M"},"source":["## Predictions of the model in the validation set\n","\n","We start by defining a predict function which takes a data loader and produces predictions for all samples.\n","\n","In addition to the predictions, this function also provides other useful outputs\n","\n","The predictions here are a categorical distribution over the 10 different digits, which means that each prediction has dimension 10. We can recover the label for which the model is most confident by taking an $\\arg\\max$ over the predicted distribution."]},{"cell_type":"code","metadata":{"id":"w6PJMaehrEC3"},"source":["def get_predictions(model, iterator, device):\n","\n","    model.eval()\n","\n","    # save the predictions, images and labels in this list\n","    y_preds = []\n","    images = []\n","    labels = []\n","    \n","    with torch.no_grad():\n","\n","        for batch, targets in iterator:\n","            batch = batch.to(device)\n","            # predict probabilities of each class\n","            predictions, _ = model(batch)\n","            # apply a softmax to the predictions\n","            predictions = ???\n","            \n","            # top_pred = y_preds.argmax(1, keepdim = True)\n","\n","            # save\n","            images.append(batch.cpu())\n","            labels.append(targets.cpu())\n","            y_preds.append(predictions.cpu())\n","\n","    # stack\n","    images = torch.cat(images, dim = 0)\n","    labels = torch.cat(labels, dim = 0)\n","    y_preds = torch.cat(y_preds, dim = 0)\n","\n","    return images, labels, y_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nzP0-he-4i_"},"source":["# compute predictions on the validation set\n","# images, labels, probs =\n","images, labels, probs = get_predictions(model, valid_loader, device)\n","\n","# ...and then get the predicted labels from the model's predictions. Use torch.argmax\n","pred_labels = ???\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87r7l8ewG6v-"},"source":["# convert to numpy\n","y_true = np.array(labels)\n","y_pred = np.array(pred_labels)\n","\n","# calculate the number of errors\n","num_errors = ???\n","\n","print(f'Validation errors {num_errors} (out of {len(valid_set)})')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSoN7mTdq_WA"},"source":["## Confusion matrix\n"," \n","We'll examine our model by plotting a confusion matrix, which helps us understand which classes the model is more frequently mixing"]},{"cell_type":"code","metadata":{"id":"sjczzk9OmT5s"},"source":["def plot_confusion_matrix(labels, pred_labels):\n","    \n","    fig = plt.figure(figsize = (10, 10));\n","    ax = fig.add_subplot(1, 1, 1);\n","    # use the confusion_matrix and ConfusionMatrixDisplay functions\n","    # from sklearn.metrics to display the confusion matrix\n","    cm = ???\n","    \n","    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKM5MvaV_jSa"},"source":["plot_confusion_matrix(labels, pred_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9Pj6JjvkjAw"},"source":["## Analysis of samples predicted wrongly\n","Here we will find the samples that the model is predicting wrongly and we will. Especifically, we will look at those samples that the model is predicting wrongly with a high score for the wrong class"]},{"cell_type":"markdown","metadata":{"id":"IeI3saG_lYvP"},"source":["Find out if each sample is correctly or wrongly predicted"]},{"cell_type":"code","metadata":{"id":"YiocAm39_Trm"},"source":["# you can use torch.eq\n","corrects = ??? "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CX7G_bNZEhyf"},"source":["Then, get all of the incorrect examples and sort them by descending confidence in their prediction."]},{"cell_type":"code","metadata":{"id":"6jGk1rLc_faC"},"source":["incorrect_examples = []\n","\n","for image, label, prob, correct in zip(images, labels, probs, corrects):\n","    if not correct:\n","        incorrect_examples.append((image, label, prob))\n","\n","incorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skuRvz1dEhyg"},"source":["We can then use this to plot the examples the model got wrong and was most confident about."]},{"cell_type":"code","metadata":{"id":"F_mELVDsrd_T"},"source":["def plot_most_incorrect(incorrect, n_images):\n","\n","    rows = int(np.sqrt(n_images))\n","    cols = int(np.sqrt(n_images))\n","\n","    fig = plt.figure(figsize = (20, 10))\n","    for i in range(rows*cols):\n","        ax = fig.add_subplot(rows, cols, i+1)\n","        image, true_label, probs = incorrect[i]\n","        true_prob = probs[true_label]\n","        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\n","        ax.imshow(image.view(28, 28).cpu().numpy(), cmap = 'bone')\n","        ax.set_title(f'true label: {true_label} ({true_prob:.3f})\\n' \\\n","                     f'pred label: {incorrect_label} ({incorrect_prob:.3f})')\n","        ax.axis('off')\n","    fig.subplots_adjust(hspace=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DG5ah9JPEhyj"},"source":["Most of the mistakes look pretty reasonable as most of the digits shown are a bit off. Models are trained to be  incredibly confident with its predictions, so it's not unreasonable for it to be incredibly confident when they are wrong."]},{"cell_type":"code","metadata":{"id":"uxPqPkYCrgGD"},"source":["N_IMAGES = 25\n","\n","plot_most_incorrect(incorrect_examples, N_IMAGES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahtWY4LzQL43"},"source":["# T-SNE representations: final and intermediate layer"]},{"cell_type":"markdown","metadata":{"id":"of9NCgP3Ehyl"},"source":["Let's move on to plotting the model's representations in lower dimensions so we can visualize them.\n","\n","We will use the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique to do this. It is a very useful dimensionality reduction techinque for the visualization of high-dimensional datasets (as it happens with the representations of a CNN). You can find more information about t-SNE here: (https://lvdmaaten.github.io/tsne/)\n","\n","First, we'll get the representations (features) from the model...\n","\n"]},{"cell_type":"code","metadata":{"id":"plZRbvc1riVT"},"source":["def get_representations(model, iterator, device):\n","\n","    model.eval()\n","\n","    outputs = []\n","    intermediates = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        \n","        for (x, y) in iterator:\n","\n","            x = x.to(device)\n","\n","            y_pred, interm_feats = model(x)\n","\n","            outputs.append(y_pred.cpu())\n","            intermediates.append(interm_feats.cpu())\n","            labels.append(y)\n","        \n","    outputs = torch.cat(outputs, dim = 0)\n","    intermediates = torch.cat(intermediates, dim = 0)\n","    labels = torch.cat(labels, dim = 0)\n","\n","    return outputs, intermediates, labels\n","    # return outputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vPRW_BIrkQT"},"source":["outputs, intermediates, labels = get_representations(model, valid_loader, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGPy9n-0WyKs"},"source":["Now, let's define the functions to get the t-SNE representations in two dimensions and plot them."]},{"cell_type":"code","metadata":{"id":"qkgOhleVrtbi"},"source":["def get_tsne(data, n_components = 2, n_images = None):\n","    if n_images is not None:\n","        data = data[:n_images]\n","    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n","    tsne_data = tsne.fit_transform(data)\n","    return tsne_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fbs1ZNOsrntL"},"source":["def plot_representations(data, labels, n_images = None):\n","    if n_images is not None:\n","        data = data[:n_images]\n","        labels = labels[:n_images]\n","    fig = plt.figure(figsize = (10, 10))\n","    ax = fig.add_subplot(111)\n","    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'tab10')\n","    handles, labels = scatter.legend_elements()\n","    legend = ax.legend(handles = handles, labels = labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTJkpqEGXELW"},"source":["Let's plot the t-SNE representations for a set of validation images. We will first plot the t-SNE representations of the outputs of the network"]},{"cell_type":"code","metadata":{"id":"2OABeIBWrvdq"},"source":["N_IMAGES = 5000\n","\n","output_tsne_data = get_tsne(outputs, n_images = N_IMAGES)\n","plot_representations(output_tsne_data, labels, n_images = N_IMAGES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WvbLe9XjEhzB"},"source":["Each of the colors represent the samples that belong to one class (ground truth label). The samples grouped in a cluster present features that are categorized by the network as belonging to the same class (most predictions will be correct). Cluster density and distancing reflect the capacity of the network to correctly identify samples from different classes. "]},{"cell_type":"markdown","metadata":{"id":"K3XUmNTV2sZB"},"source":["Now let's check if intermediate representations would be enough to  obtain a correct classification of validation samples"]},{"cell_type":"code","metadata":{"id":"NMddFrKCryQq"},"source":["intermediate_tsne_data = get_tsne(intermediates, n_images = N_IMAGES)\n","plot_representations(intermediate_tsne_data, labels, n_images = N_IMAGES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3k9cOWWS8en"},"source":["# Resuming training\n","\n","Let's load up the final model and resume training to see how far we can push up the accuracy."]},{"cell_type":"code","metadata":{"id":"vqUkPWZnW9Vi"},"source":["ls checkpoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncKeZmnNoAeD"},"source":["# Load the checkpoint corresponding to the 10th epoch\n","???\n","\n","print('Resuming training from epoch', epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9XfbK_wS-3v"},"source":["# Resume training from the 10th epoch\n","# train for 10 more epochs\n","train_losses, valid_losses = ???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfRiDrmJ0UDi"},"source":["Plot the learning curves. "]},{"cell_type":"code","metadata":{"id":"kJ33E08Goc4i"},"source":["# Use the previous code\n","???"],"execution_count":null,"outputs":[]}]}